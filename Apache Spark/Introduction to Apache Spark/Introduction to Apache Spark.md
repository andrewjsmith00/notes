# An Introduction to [[Apache Spark]]
## Spark's Basic Architecture
Spark manages and coordinates the execution of tasks on data across a cluster of computers to increase the speed of data processing. The cluster that Spark will use is managed by a cluster manager such as Spark's standalone cluster manager, [[YARN]] or [[Apache Mesos]]. Spark Applications are submitted to these managers which will grant the resources to complete the work.

## [[Spark Applications]]
## [[Spark's Language APIs]]
## [[Starting Spark]]
## The [[SparkSession]]
## [[DataFrames]]
## [[Transformations]]
## [[Actions]]
## [[Spark UI]]

## An end-to-end example