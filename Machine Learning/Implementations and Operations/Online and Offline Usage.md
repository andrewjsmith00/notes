## Online Usage
	Make inference on demand and return results

## Offline model
	Make inference in batch and return result
